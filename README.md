# **mT5 Hindi â†” Bhili Translation Models**

This repository contains code and resources for fine-tuning the **mT5-base** and **mT5-small** models for **Hindi-to-Bhili** and **Bhili-to-Hindi** translation. These models have been trained on a custom dataset of parallel Hindi-Bhili sentences.

---

## ğŸ“– **Model Details**

### **1ï¸âƒ£ mT5-Base**
- **Base Model**: [google/mt5-base](https://huggingface.co/google/mt5-base)
- **Type**: Sequence-to-sequence (Seq2Seq) transformer
- **Parameters**: ~580M
- **Fine-tuning**: Optimized for Hindi â†” Bhili translation

### **2ï¸âƒ£ mT5-Small**
- **Base Model**: [google/mt5-small](https://huggingface.co/google/mt5-small)
- **Type**: Sequence-to-sequence (Seq2Seq) transformer
- **Parameters**: ~300M
- **Fine-tuning**: Optimized for Hindi â†” Bhili translation

---

## ğŸ“‚ **Dataset**

- **Size**: 133,500 parallel Hindi-Bhili sentence pairs
- **Source**: Custom dataset
- **Usage**: Supports both **Hindi â†’ Bhili** and **Bhili â†’ Hindi** translations

---

## ğŸš€ **Training Process**

### ** Preprocessing**
- **Tokenizer**: `AutoTokenizer.from_pretrained("google/mt5-small")` or `AutoTokenizer.from_pretrained("google/mt5-base")`
- **Max Sequence Length**: 128 tokens
- **Truncation & Padding**: Applied

## ğŸ“Š **Evaluation Metrics**

### **mT5-Base Performance**
| **Metric**  | **Score**  |
|------------|-----------|
| **BLEU**   | `11.21`   |
| **chrF**   | `42.80`   |
| **chrF++** | `40.92`   |

### **mT5-Small Performance**
| **Metric**  | **Score**  |
|------------|-----------|
| **BLEU**   | `10.45`   |
| **chrF**   | `41.60`   |
| **chrF++** | `40.05`   |

---

## ğŸ“¥ **Model Saving & Deployment**

After fine-tuning, the trained models and tokenizers are saved in:

ğŸ“‚ `./mt5-small-hi-bhili-finetuned/`  *(for mT5-Small)*  
ğŸ“‚ `./mt5-base-hi-bhili-finetuned/`  *(for mT5-Base)*  

---

## ğŸ–¥ï¸ **Inference Example**

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    return tokenizer, model

def translate(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    output = model.generate(**inputs, max_length=128)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Load mT5-Small model
tokenizer, model = load_model("./mt5-small-hi-bhili-finetuned")
print(translate("à¤†à¤ªà¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ", tokenizer, model))  # Expected Output: "à¤¤à¥à¤®à¤¾à¤°à¥‹ à¤¸à¥à¤—à¤¤à¥‹ à¤šà¥‡"

# Load mT5-Base model
tokenizer, model = load_model("./mt5-base-hi-bhili-finetuned")
print(translate("à¤†à¤ªà¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ", tokenizer, model))
```

---

## ğŸ™Œ **Acknowledgments**

- **Google** for the `mT5` models
- **Hugging Face** for the `Transformers` and `Datasets` library

---
